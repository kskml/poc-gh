import os
import json
import hashlib
import tiktoken
from openai import AzureOpenAI
from dotenv import load_dotenv
import datetime

# ==========================================
# 1. SETUP & CONFIGURATION
# ==========================================

def get_azure_client():
    """Initializes and returns the Azure OpenAI client."""
    load_dotenv()
    
    # Prompt Caching requires API version 2024-08-01-preview or later
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version="2024-08-01-preview", 
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    return client

# ==========================================
# 2. FILE SYSTEM OPERATIONS
# ==========================================

def read_source_code_files(source_dir, extensions=[".py", ".js", ".java", ".cs", ".ts"]):
    """Recursively reads all files."""
    code_files = {}
    raw_content_for_hash = ""
    
    if not os.path.exists(source_dir):
        raise FileNotFoundError(f"Source directory not found: {source_dir}")

    for root, dirs, files in os.walk(source_dir):
        dirs[:] = [d for d in dirs if d not in ['venv', '__pycache__', 'node_modules', '.git', 'env', 'build', 'dist']]
        
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        rel_path = os.path.relpath(file_path, source_dir)
                        code_files[rel_path] = content
                        raw_content_for_hash += f"FILE:{rel_path}\n{content}\n"
                except Exception as e:
                    print(f"Warning: Could not read {file_path}: {e}")
                    
    return code_files, raw_content_for_hash

def read_hld_file(hld_path):
    if not os.path.exists(hld_path):
        raise FileNotFoundError(f"HLD file not found: {hld_path}")
    with open(hld_path, 'r', encoding='utf-8') as f:
        return f.read()

# ==========================================
# 3. DATA FORMATTING STRATEGIES
# ==========================================

def format_json_payload(code_files_dict):
    """Strategy A: Standard Minified JSON."""
    # separators=(',', ':') creates the most compact JSON string
    return json.dumps(code_files_dict, separators=(',', ':'))

def format_custom_toon_payload(code_files_dict):
    """Strategy B: Optimized TOON (Markdown Section Style)."""
    # This is better than the 'toon' library for code files.
    # It avoids escaping newlines, saving tokens.
    if not code_files_dict:
        return "No data provided"
    
    rows = []
    for filename, content in code_files_dict.items():
        # Using markdown headers creates a clean, token-efficient structure
        rows.append(f"### FILE: {filename}")
        rows.append(content)
            
    return "\n".join(rows)

# ==========================================
# 4. TOKEN COUNTING UTILITIES
# ==========================================

def count_tokens_tiktoken(text, model="gpt-4"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# ==========================================
# 5. CACHING LOGIC
# ==========================================

def generate_cache_key(strategy_name, source_raw, hld_raw):
    combined = f"{strategy_name}||{source_raw}||{hld_raw}"
    return hashlib.sha256(combined.encode('utf-8')).hexdigest()

def get_cached_response(cache_key, cache_dir="llm_cache"):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return None

def save_to_cache(cache_key, response_data, cache_dir="llm_cache"):
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    with open(file_path, 'w') as f:
        json.dump(response_data, f)

# ==========================================
# 6. CORE ANALYSIS LOGIC
# ==========================================

def run_analysis(client, deployment_name, strategy_name, code_files_dict, code_raw_content, hld_content, force_bypass_local_cache=False):
    """
    Runs the Gap Analysis.
    force_bypass_local_cache: Used to test Azure Cache (Local Miss -> Azure Hit)
    """
    print(f"\n--- Running Analysis: {strategy_name} ---")
    
    # 1. Prepare Payload
    if strategy_name == "JSON_STRATEGY":
        payload_str = format_json_payload(code_files_dict)
        system_prompt = "You are an expert code auditor. Analyze the JSON source code against the HLD. Output a gap analysis."
    else:
        payload_str = format_custom_toon_payload(code_files_dict)
        system_prompt = "You are an expert code auditor. Analyze the TOON formatted source code against the HLD. Output a gap analysis."

    # 2. Local Cache Logic
    cache_key = generate_cache_key(strategy_name, code_raw_content, hld_content)
    cached_result = None
    
    if not force_bypass_local_cache:
        cached_result = get_cached_response(cache_key)
    
    metrics = {
        'strategy': strategy_name,
        'local_cache_hit': False,
        'azure_cache_hit': False,
        'prompt_tokens': 0,
        'completion_tokens': 0,
        'total_tokens': 0,
        'azure_cached_tokens': 0
    }

    if cached_result:
        print(f"{strategy_name}: Local Cache HIT.")
        usage = cached_result.get("usage", {})
        metrics['local_cache_hit'] = True
        metrics['prompt_tokens'] = usage.get('prompt_tokens', 0)
        metrics['completion_tokens'] = usage.get('completion_tokens', 0)
        metrics['total_tokens'] = usage.get('total_tokens', 0)
        metrics['azure_cached_tokens'] = usage.get('azure_cached_tokens', 0)
        metrics['azure_cache_hit'] = metrics['azure_cached_tokens'] > 0
        return cached_result.get("response"), metrics
    
    print(f"{strategy_name}: Local Cache MISS. Calling Azure...")
    
    # 3. Construct Prompt for Azure
    # Structure: System -> Context (Code+HLD) -> Instructions
    user_content = f"# HLD Documentation:\n{hld_content}\n\n# Source Code:\n{payload_str}"
    
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.0 
        )
        
        response_text = response.choices[0].message.content
        usage = response.usage
        
        # Extract Azure Cache Metrics
        azure_cached_tokens = 0
        if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
            azure_cached_tokens = usage.prompt_tokens_details.cached_tokens or 0
        
        metrics['prompt_tokens'] = usage.prompt_tokens
        metrics['completion_tokens'] = usage.completion_tokens
        metrics['total_tokens'] = usage.total_tokens
        metrics['azure_cached_tokens'] = azure_cached_tokens
        metrics['azure_cache_hit'] = azure_cached_tokens > 0
        
        # Save to Local Cache
        save_to_cache(cache_key, {
            "response": response_text, 
            "usage": {
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens,
                "azure_cached_tokens": azure_cached_tokens
            }
        })
        
        return response_text, metrics
        
    except Exception as e:
        print(f"Error in {strategy_name}: {str(e)}")
        return f"Error: {str(e)}", metrics

# ==========================================
# 7. REPORT GENERATION
# ==========================================

def generate_comparison_report(report_data, output_file="comparison_report.md"):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Extract Metrics
    json_cold = report_data['json_cold']
    json_hot = report_data['json_hot']   
    toon_cold = report_data['toon_cold']
    toon_hot = report_data['toon_hot']   
    
    # Calculations
    token_reduction = json_cold['prompt_tokens'] - toon_cold['prompt_tokens']
    pct_reduction = (token_reduction / json_cold['prompt_tokens']) * 100 if json_cold['prompt_tokens'] > 0 else 0

    content = f"""# Gap Analysis Comparison Report

**Date:** {now}

---

## 1. Strategy Comparison: JSON vs. TOON (Cold Start)
This compares the first-time execution cost (no cache used).

| Metric | JSON Strategy | TOON Strategy | Savings |
| :--- | :---: | :---: | :---: |
| **Input Tokens** | {json_cold['prompt_tokens']} | {toon_cold['prompt_tokens']} | **-{token_reduction} ({pct_reduction:.2f}%)** |
| **Output Tokens** | {json_cold['completion_tokens']} | {toon_cold['completion_tokens']} | - |
| **Total Tokens** | {json_cold['total_tokens']} | {toon_cold['total_tokens']} | - |

---

## 2. Azure Prompt Caching Demonstration
Azure Prompt Caching activates automatically when the prefix is > 1024 tokens and identical.

### JSON Strategy Caching
| Run | Input Tokens | Cached Tokens | Status |
| :--- | :---: | :---: | :--- |
| **Cold** | {json_cold['prompt_tokens']} | {json_cold['azure_cached_tokens']} | {'Active' if json_cold['azure_cache_hit'] else 'Inactive'} |
| **Hot** | {json_hot['prompt_tokens']} | {json_hot['azure_cached_tokens']} | {'**ACTIVE**' if json_hot['azure_cache_hit'] else 'Inactive'} |

### TOON Strategy Caching
| Run | Input Tokens | Cached Tokens | Status |
| :--- | :---: | :---: | :--- |
| **Cold** | {toon_cold['prompt_tokens']} | {toon_cold['azure_cached_tokens']} | {'Active' if toon_cold['azure_cache_hit'] else 'Inactive'} |
| **Hot** | {toon_hot['prompt_tokens']} | {toon_hot['azure_cached_tokens']} | {'**ACTIVE**' if toon_hot['azure_cache_hit'] else 'Inactive'} |

---

## 3. Conclusion
1. **TOON Optimization**: Reduced input tokens by **{pct_reduction:.2f}%** compared to minified JSON.
2. **Azure Caching**: Successfully demonstrated in "Hot" runs. Notice high `Cached Tokens` count.
3. **Library Note**: This app uses a custom TOON generator optimized for code, rather than the `toon-format` library, to ensure maximum token efficiency for multi-line strings.

## 4. Gap Analysis Result (Preview)
