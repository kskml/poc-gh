import os
import logging
import json
from openai import AzureOpenAI
from github import Github, GithubException
from typing import List, Dict, Any
import concurrent.futures

# Configure standard logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
def get_config() -> Dict[str, str]:
    """Loads configuration from environment variables."""
    return {
        "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
        "azure_api_key": os.getenv("AZURE_OPENAI_API_KEY"),
        "deployment_name": os.getenv("AZURE_DEPLOYMENT_NAME"),
        "gh_token": os.getenv("GH_TOKEN"),
        "docs_repo": os.getenv("DOCS_REPO_NAME"),
        "source_pr_number": os.getenv("SOURCE_PR_NUMBER")
    }

# --- Clients ---
def get_azure_client(config: Dict[str, str]) -> AzureOpenAI:
    return AzureOpenAI(
        api_key=config["azure_api_key"],
        api_version="2024-02-15-preview",
        azure_endpoint=config["azure_endpoint"]
    )

def get_github_client(config: Dict[str, str]) -> Github:
    return Github(config["gh_token"])

# -----------------------------------------------------------------------
# PHASE 1: BATCHED ANALYSIS (Filtering)
# -----------------------------------------------------------------------

def chunk_list(data: List[Any], chunk_size: int) -> List[List[Any]]:
    """Helper to split a list into chunks."""
    for i in range(0, len(data), chunk_size):
        yield data[i:i + chunk_size]

def analyze_doc_batch(diff: str, doc_batch: List[Dict[str, str]], client: AzureOpenAI, deployment: str) -> List[Dict[str, Any]]:
    """
    Step 1: Analyze a batch of docs to identify gaps.
    Returns list of relevant files with reasoning and proposed changes (summary).
    """
    logger.info(f"Analyzing batch of {len(doc_batch)} docs...")
    
    # Construct the content string for the prompt
    docs_content_str = ""
    for doc in doc_batch:
        docs_content_str += f"\n--- FILE: {doc['path']} ---\n{doc['content']}\n"

    system_prompt = """
    You are a code documentation auditor. Compare the 'Code Diff' against the provided 'Documentation Files'.
    
    Your Goal: Identify which files are outdated, missing info, or contain gaps.
    
    For each file that needs updating, provide:
    1. 'file_path': The exact path of the file.
    2. 'reasoning': Why it needs updating.
    3. 'proposed_ascii_changes': A summary of what needs to be changed (bullet points).
    
    DO NOT generate the full updated file content here. Just identify the gaps.
    
    Return a JSON object:
    {
        "affected_files": [
            {
                "file_path": "...",
                "reasoning": "...",
                "proposed_ascii_changes": "..."
            }
        ]
    }
    """
    
    user_prompt = f"""
    Code Diff:
    {diff}
    
    Documentation Files:
    {docs_content_str}
    """

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        result = json.loads(response.choices[0].message.content)
        return result.get("affected_files", [])
    except Exception as e:
        logger.error(f"Error in batch analysis: {e}")
        return []

def identify_relevant_docs_batched(diff: str, all_docs_map: Dict[str, str], client: AzureOpenAI, deployment: str) -> List[Dict[str, Any]]:
    """
    Splits docs into batches and processes them in parallel.
    """
    logger.info("Phase 1: Batched Analysis of all docs...")
    
    # Convert dict to list of dicts for processing
    docs_list = [{"path": k, "content": v} for k, v in all_docs_map.items()]
    
    # Define batch size (sending 3 docs per call balances context window and speed)
    BATCH_SIZE = 3
    batches = list(chunk_list(docs_list, BATCH_SIZE))
    
    all_affected_files = []
    
    # Process batches in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_batch = {
            executor.submit(analyze_doc_batch, diff, batch, client, deployment): batch 
            for batch in batches
        }
        
        for future in concurrent.futures.as_completed(future_to_batch):
            try:
                results = future.result()
                all_affected_files.extend(results)
            except Exception as exc:
                logger.error(f"Batch generated an exception: {exc}")

    logger.info(f"Phase 1 Complete. Identified {len(all_affected_files)} files needing updates.")
    return all_affected_files

# -----------------------------------------------------------------------
# PHASE 2: INDIVIDUAL REFACTORING
# -----------------------------------------------------------------------

def refactor_single_file(diff: str, file_meta: Dict[str, Any], all_docs_map: Dict[str, str], client: AzureOpenAI, deployment: str) -> Dict[str, Any]:
    """
    Step 2: Generates the full updated content for a single specific file.
    """
    path = file_meta['file_path']
    logger.info(f"Phase 2: Refactoring {path}...")
    
    current_content = all_docs_map.get(path, "")
    proposed_changes = file_meta.get('proposed_ascii_changes', "")

    system_prompt = """
    You are a technical writer. Update the provided documentation based on the Code Diff and Proposed Changes.
    
    Task: Rewrite the 'Original Content' to reflect the necessary changes.
    Ensure the output is a complete, valid ASCII document.
    
    Return a JSON object:
    {
        "updated_file_content": "The full text of the updated document..."
    }
    """
    
    user_prompt = f"""
    File Path: {path}
    
    Code Diff (Context):
    {diff}
    
    Proposed Changes (Summary):
    {proposed_changes}
    
    Original Content to Update:
    {current_content}
    """

    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        result = json.loads(response.choices[0].message.content)
        
        return {
            "file_path": path,
            "reasoning": file_meta.get('reasoning'),
            "proposed_changes": proposed_changes,
            "updated_file_content": result.get("updated_file_content", current_content)
        }
    except Exception as e:
        logger.error(f"Error refactoring {path}: {e}")
        return None

# -----------------------------------------------------------------------
# PHASE 3: GITHUB PR
# -----------------------------------------------------------------------

def create_docs_pr(updated_files: List[Dict[str, str]], config: Dict[str, str]) -> None:
    if not updated_files:
        logger.info("No files to update.")
        return

    gh_client = get_github_client(config)
    repo_name = config["docs_repo"]
    
    try:
        repo = gh_client.get_repo(repo_name)
        
        # Branch Logic
        base_branch = repo.default_branch
        branch_suffix = config.get("source_pr_number", "update").replace("/", "-")
        new_branch_name = f"docs-update-{branch_suffix}"
        
        # Cleanup old branch if exists
        try:
            repo.get_git_ref(f'refs/heads/{new_branch_name}').delete()
        except GithubException:
            pass

        base_ref = repo.get_git_ref(f'heads/{base_branch}')
        repo.create_git_ref(ref=f'refs/heads/{new_branch_name}', sha=base_ref.object.sha)
        logger.info(f"Created branch {new_branch_name}")

        # Commit Loop
        pr_body_lines = ["### Automated Documentation Updates\n"]
        
        for file_data in updated_files:
            path = file_data["file_path"]
            new_content = file_data["updated_file_content"]
            
            try:
                # Update existing
                file_ref = repo.get_contents(path, ref=new_branch_name)
                repo.update_file(
                    path=path,
                    message=f"Update {path}",
                    content=new_content,
                    sha=file_ref.sha,
                    branch=new_branch_name
                )
                logger.info(f"Updated: {path}")
            except GithubException as e:
                if e.status == 404:
                    # Create new
                    repo.create_file(
                        path=path,
                        message=f"Create {path}",
                        content=new_content,
                        branch=new_branch_name
                    )
                    logger.info(f"Created: {path}")
                else:
                    raise e
            
            # Append to PR description
            pr_body_lines.append(
                f"**File:** `{path}`\n"
                f"- **Reason:** {file_data['reasoning']}\n"
                f"- **Changes:** {file_data['proposed_changes']}\n"
            )

        # Create PR
        pr = repo.create_pull(
            title=f"Docs Update (Source PR #{config.get('source_pr_number', 'N/A')})",
            body="\n".join(pr_body_lines),
            head=new_branch_name,
            base=base_branch
        )
        logger.info(f"PR Created: {pr.html_url}")

    except Exception as e:
        logger.error(f"PR Creation Failed: {e}")
        raise

# -----------------------------------------------------------------------
# MAIN ORCHESTRATOR
# -----------------------------------------------------------------------

def run_docs_sync_job(pr_diff: str, ascii_docs_map: Dict[str, str]):
    config = get_config()
    client = get_azure_client(config)
    deployment = config["deployment_name"]

    logger.info("Starting Batched Docs Sync Job...")

    # 1. Batched Analysis: Identify gaps for multiple docs at once (Step 1)
    affected_files_meta = identify_relevant_docs_batched(pr_diff, ascii_docs_map, client, deployment)

    # 2. Individual Refactoring: Generate full content for affected docs (Step 2)
    fully_updated_files = []
    
    # We run this sequentially or with limited parallelism to ensure quality
    for file_meta in affected_files_meta:
        updated_data = refactor_single_file(pr_diff, file_meta, ascii_docs_map, client, deployment)
        if updated_data:
            fully_updated_files.append(updated_data)

    # 3. Create PR
    create_docs_pr(fully_updated_files, config)
    logger.info("Job Finished.")

# --- Mock Execution ---
if __name__ == "__main__":
    # Mock Data
    mock_diff = """
    diff --git a/auth.py b/auth.py
    --- a/auth.py
    +++ b/auth.py
    @@ -5 +5 @@
    -def login(u): pass
    +def login(u, mfa=False): pass
    """
    
    mock_docs = {
        "docs/guide.ascii": "# Guide\nLogin uses `login(u)`.",
        "docs/api.ascii": "# API\nDetails about networking.",
        "readme.asc": "# Welcome\nTo the project."
    }
    
    # run_docs_sync_job(mock_diff, mock_docs)
