import os
import json
import base64
import requests
from openai import OpenAI

# --- Configuration & Clients ---
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github+json",
    "X-GitHub-Api-Version": "2022-11-28"
}

openai_client = OpenAI(api_key=OPENAI_API_KEY)

# --- Helper: Generic GitHub REST Calls ---

def github_get(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 404:
        return None
    response.raise_for_status()
    return response.json()

def github_put(url, data):
    response = requests.put(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

def github_post(url, data):
    response = requests.post(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

# --- Step 1: Data Ingestion ---

def get_pr_data(repo_name, pr_number):
    url = f"https://api.github.com/repos/{repo_name}/pulls/{pr_number}"
    pr_data = github_get(url)
    
    diff_response = requests.get(pr_data["diff_url"])
    diff_response.raise_for_status()
    
    return {
        "diff": diff_response.text,
        "base_branch": pr_data["base"]["ref"],
        "head_branch": pr_data["head"]["ref"],
        "head_sha": pr_data["head"]["sha"]
    }

def get_all_adoc_files(repo_name, branch_ref):
    tree_url = f"https://api.github.com/repos/{repo_name}/git/trees/{branch_ref}?recursive=1"
    tree_data = github_get(tree_url)
    
    docs_map = {}
    if not tree_data: return docs_map

    for item in tree_data.get("tree", []):
        if item["type"] == "blob" and item["path"].endswith(".adoc"):
            content_url = f"https://api.github.com/repos/{repo_name}/contents/{item['path']}?ref={branch_ref}"
            try:
                file_data = github_get(content_url)
                if file_data:
                    decoded_content = base64.b64decode(file_data["content"]).decode('utf-8')
                    docs_map[item["path"]] = decoded_content
            except Exception as e:
                print(f"Warning: Could not read {item['path']}: {e}")
                
    return docs_map

# --- Step 2: LLM Analysis ---

def chunk_dict(data, chunk_size=3):
    items = list(data.items())
    for i in range(0, len(items), chunk_size):
        yield dict(items[i:i + chunk_size])

def analyze_diff_in_batches(code_diff, docs_map, batch_size=3):
    all_proposed_changes = []
    batches = list(chunk_dict(docs_map, batch_size))
    
    print(f"Analyzing {len(docs_map)} files in {len(batches)} batches...")

    for i, batch in enumerate(batches):
        batch_context = "\n\n".join([f"File: {path}\nContent:\n{content}" for path, content in batch.items()])

        prompt = f"""
        Analyze Code Diff vs Documentation.
        Code Diff: {code_diff}
        Docs: {batch_context}
        Return JSON {{"updates": [{{"filename": "...", "proposed_changes": "..."}}]}}.
        If no changes, return empty list.
        """

        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            if "updates" in result:
                valid_updates = [u for u in result["updates"] if u.get("filename") and u.get("proposed_changes")]
                all_proposed_changes.extend(valid_updates)
                
        except Exception as e:
            print(f"Error in batch {i+1}: {e}")

    return all_proposed_changes

# --- Step 3: Refactoring (New Flow) ---

def clean_adoc_content(content):
    """Removes common LLM artifacts like Markdown code blocks."""
    content = content.strip()
    if content.startswith("```asciidoc"):
        content = content[len("```asciidoc"):]
    if content.startswith("```"):
        content = content[len("```"):]
    if content.endswith("```"):
        content = content[:-len("```")]
    return content.strip()

def refactor_identified_file(original_content, proposed_changes, code_diff):
    """
    Instructs LLM to apply minimal updates.
    """
    prompt = f"""
    You are an AsciiDoc specialist. Your task is to update an existing document based on specific instructions.
    
    # INSTRUCTIONS
    1.  Apply the 'Proposed Changes' to the 'Original Document'.
    2.  **CRITICAL**: Do NOT rewrite the entire document. Keep the existing structure and unchanged parts exactly as they are. Only modify the parts necessary to satisfy the 'Proposed Changes'.
    3.  Output MUST be valid AsciiDoc syntax.
    4.  Do NOT use Markdown formatting (no ``` blocks). Return raw text only.
    
    # Original Document
    {original_content}
    
    # Proposed Changes
    {proposed_changes}
    
    # Context (Code Diff)
    {code_diff}
    
    # Updated Document (Raw AsciiDoc only)
    """

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        return clean_adoc_content(response.choices[0].message.content)
    except Exception as e:
        print(f"LLM Refactoring error: {e}")
        return None

def refactor_all_files(proposed_changes, repo_name, head_branch, code_diff):
    """
    New Flow Step:
    1. Iterates through identified changes.
    2. Fetches content and calls LLM for each.
    3. Stores results in a dictionary: {filename: new_content}
    """
    refactored_files_dict = {}
    
    print(f"\n--- Phase: Refactoring {len(proposed_changes)} Files ---")

    for change in proposed_changes:
        filename = change['filename']
        instruction = change['proposed_changes']
        
        print(f"Refactoring: {filename}")
        
        try:
            # 1. Fetch current content from the PR branch
            url = f"https://api.github.com/repos/{repo_name}/contents/{filename}?ref={head_branch}"
            file_data = github_get(url)
            if not file_data:
                print(f"File {filename} not found, skipping.")
                continue
                
            original_content = base64.b64decode(file_data["content"]).decode('utf-8')
            
            # 2. Call LLM to get refactored content
            new_content = refactor_identified_file(original_content, instruction, code_diff)
            
            # 3. Store in dictionary if valid
            if new_content:
                # Optional: Check if content actually changed to avoid unnecessary commits later
                if new_content != original_content:
                    refactored_files_dict[filename] = new_content
                else:
                    print(f"Skipping {filename}: LLM returned identical content.")
                    
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            
    return refactored_files_dict

# --- Step 4: GitHub PR Creation ---

def get_ref_sha(repo_name, ref):
    url = f"https://api.github.com/repos/{repo_name}/git/ref/heads/{ref}"
    data = github_get(url)
    return data["object"]["sha"]

def create_branch(repo_name, new_branch_name, source_sha):
    url = f"https://api.github.com/repos/{repo_name}/git/refs"
    data = {"ref": f"refs/heads/{new_branch_name}", "sha": source_sha}
    try:
        github_post(url, data)
        print(f"Created new branch: {new_branch_name}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print(f"Branch '{new_branch_name}' already exists. Reusing it.")
        else:
            raise e

def commit_file(repo_name, file_path, new_content, target_branch):
    # Fetch SHA from the target branch (handles existing files or updates)
    url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}?ref={target_branch}"
    file_data = github_get(url)
    
    current_sha = None
    if file_data and "content" in file_data:
        current_sha = file_data["sha"]
        # Double check content isn't identical (Safety check)
        current_content = base64.b64decode(file_data["content"]).decode('utf-8')
        if new_content == current_content:
            return False

    commit_url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
    data = {
        "message": f"docs: update {file_path}",
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": current_sha,
        "branch": target_branch
    }
    
    github_put(commit_url, data)
    print(f"Committed: {file_path}")
    return True

def raise_pull_request(repo_name, head_branch, refactored_files_dict, original_pr_number):
    """
    Step 2 of New Flow:
    Takes the dictionary of refactored files and creates a PR.
    """
    if not refactored_files_dict:
        print("No refactored files to commit. Exiting.")
        return

    new_branch_name = f"doc-update/pr-{original_pr_number}-auto"
    
    # 1. Create Branch
    source_sha = get_ref_sha(repo_name, head_branch)
    create_branch(repo_name, new_branch_name, source_sha)

    # 2. Commit Changes from Dictionary
    commits_made = 0
    for file_path, new_content in refactored_files_dict.items():
        try:
            success = commit_file(repo_name, file_path, new_content, new_branch_name)
            if success:
                commits_made += 1
        except Exception as e:
            print(f"Error committing {file_path}: {e}")

    # 3. Create PR
    if commits_made == 0:
        print("No new commits made (files were identical). PR not created.")
        return

    pr_url = f"https://api.github.com/repos/{repo_name}/pulls"
    pr_data = {
        "title": f"[Auto-Generated] Documentation updates for PR #{original_pr_number}",
        "body": f"Automated updates based on code changes in PR #{original_pr_number}.",
        "head": new_branch_name,
        "base": head_branch
    }
    
    try:
        pr_response = github_post(pr_url, pr_data)
        print(f"\nSuccess! Pull Request created: {pr_response['html_url']}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print("PR already exists or validation failed.")
        else:
            raise e

# --- Main Orchestrator ---

def main(repo_name, pr_number):
    print(f"--- Starting Flow for {repo_name} PR #{pr_number} ---")

    # Step 1: Fetch Data
    print("\nStep 1: Fetching Data...")
    pr_info = get_pr_data(repo_name, pr_number)
    docs_map = get_all_adoc_files(repo_name, pr_info["head_branch"])
    
    if not docs_map:
        print("No AsciiDoc files found.")
        return

    # Step 2: Analysis
    print("\nStep 2: Analyzing Gaps...")
    proposed_changes = analyze_diff_in_batches(pr_info["diff"], docs_map)
    
    if not proposed_changes:
        print("No gaps found.")
        return

    # Step 3: Refactor All Files (Store in Dict)
    print("\nStep 3: Refactoring Files...")
    refactored_files = refactor_all_files(proposed_changes, repo_name, pr_info["head_branch"], pr_info["diff"])
    
    # Step 4: Raise PR (Use Dict)
    print("\nStep 4: Creating PR...")
    raise_pull_request(repo_name, pr_info["head_branch"], refactored_files, pr_number)

if __name__ == "__main__":
    REPO = "owner/repo-name"
    PR_NUM = 12
    
    if not GITHUB_TOKEN or not OPENAI_API_KEY:
        print("Set GITHUB_TOKEN and OPENAI_API_KEY env vars.")
    else:
        main(REPO, PR_NUM)
