import os
import json
import base64
import requests
from openai import OpenAI

# --- Configuration & Clients ---
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github+json",
    "X-GitHub-Api-Version": "2022-11-28"
}

openai_client = OpenAI(api_key=OPENAI_API_KEY)

# --- Helper: Generic GitHub REST Calls ---

def github_get(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 404:
        return None # Handle file not found gracefully
    response.raise_for_status()
    return response.json()

def github_put(url, data):
    response = requests.put(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

def github_post(url, data):
    response = requests.post(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

# --- Step 1: Data Ingestion ---

def get_pr_data(repo_name, pr_number):
    url = f"https://api.github.com/repos/{repo_name}/pulls/{pr_number}"
    pr_data = github_get(url)
    
    diff_response = requests.get(pr_data["diff_url"])
    diff_response.raise_for_status()
    
    return {
        "diff": diff_response.text,
        "base_branch": pr_data["base"]["ref"],
        "head_branch": pr_data["head"]["ref"],
        "head_sha": pr_data["head"]["sha"]
    }

def get_all_adoc_files(repo_name, branch_ref):
    tree_url = f"https://api.github.com/repos/{repo_name}/git/trees/{branch_ref}?recursive=1"
    tree_data = github_get(tree_url)
    
    docs_map = {}
    if not tree_data:
        return docs_map

    for item in tree_data.get("tree", []):
        if item["type"] == "blob" and item["path"].endswith(".adoc"):
            content_url = f"https://api.github.com/repos/{repo_name}/contents/{item['path']}?ref={branch_ref}"
            try:
                file_data = github_get(content_url)
                if file_data:
                    decoded_content = base64.b64decode(file_data["content"]).decode('utf-8')
                    docs_map[item["path"]] = decoded_content
            except Exception as e:
                print(f"Warning: Could not read {item['path']}: {e}")
                
    return docs_map

# --- Step 2: LLM Analysis ---

def chunk_dict(data, chunk_size=3):
    items = list(data.items())
    for i in range(0, len(items), chunk_size):
        yield dict(items[i:i + chunk_size])

def analyze_diff_in_batches(code_diff, docs_map, batch_size=3):
    all_proposed_changes = []
    batches = list(chunk_dict(docs_map, batch_size))
    
    print(f"Analyzing {len(docs_map)} files in {len(batches)} batches...")

    for i, batch in enumerate(batches):
        batch_context = "\n\n".join([f"File: {path}\nContent:\n{content}" for path, content in batch.items()])

        prompt = f"""
        Analyze Code Diff vs Documentation.
        Code Diff: {code_diff}
        Docs: {batch_context}
        Return JSON {{"updates": [{{"filename": "...", "proposed_changes": "..."}}]}}.
        If no changes, return empty list.
        """

        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            if "updates" in result:
                valid_updates = [u for u in result["updates"] if u.get("filename") and u.get("proposed_changes")]
                all_proposed_changes.extend(valid_updates)
                
        except Exception as e:
            print(f"Error in batch {i+1}: {e}")

    return all_proposed_changes

# --- Step 3: Refactoring ---

def refactor_identified_file(original_content, proposed_changes, code_diff):
    prompt = f"""
    Rewrite the following AsciiDoc file based on proposed changes.
    
    Original:
    {original_content}
    
    Changes: {proposed_changes}
    Context Diff: {code_diff}
    
    Return COMPLETE updated content only.
    """
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"LLM error: {e}")
        return None

def process_refactoring(proposed_changes, repo_name, head_branch, code_diff):
    """
    Step 3: Generates new content.
    Note: We do NOT fetch SHA here. We will fetch it later during the commit step
    to ensure we have the correct SHA for the branch we are committing to.
    """
    refactored_files = []
    
    print(f"\nRefactoring {len(proposed_changes)} identified files...")

    for change in proposed_changes:
        filename = change['filename']
        instruction = change['proposed_changes']
        
        print(f"Generating content for: {filename}")
        
        try:
            # 1. Fetch content from the HEAD branch to feed to LLM
            # We use the head_branch (source PR branch) to get the "original" content
            url = f"https://api.github.com/repos/{repo_name}/contents/{filename}?ref={head_branch}"
            file_data = github_get(url)
            if not file_data:
                print(f"File {filename} not found, skipping.")
                continue
                
            original_content = base64.b64decode(file_data["content"]).decode('utf-8')
            
            # 2. LLM Generation
            new_content = refactor_identified_file(original_content, instruction, code_diff)
            
            if new_content:
                refactored_files.append({
                    "path": filename,
                    "new_content": new_content
                    # SHA is NOT stored here
                })
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            
    return refactored_files

# --- Step 4: GitHub PR Creation ---

def get_ref_sha(repo_name, ref):
    """Get SHA of a branch/ref."""
    url = f"https://api.github.com/repos/{repo_name}/git/ref/heads/{ref}"
    data = github_get(url)
    return data["object"]["sha"]

def create_branch(repo_name, new_branch_name, source_sha):
    url = f"https://api.github.com/repos/{repo_name}/git/refs"
    data = {"ref": f"refs/heads/{new_branch_name}", "sha": source_sha}
    try:
        github_post(url, data)
        print(f"Created new branch: {new_branch_name}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print(f"Branch '{new_branch_name}' already exists. Reusing it.")
        else:
            raise e

def commit_file(repo_name, file_path, new_content, target_branch):
    """
    Commits a file to the target branch.
    Fetches the latest SHA from the target branch right before updating.
    """
    # 1. Get the current file state on the TARGET branch (doc-update branch)
    # This handles both new files and files already modified in previous runs.
    url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}?ref={target_branch}"
    file_data = github_get(url)
    
    current_content = ""
    current_sha = None
    
    if file_data and "content" in file_data:
        current_sha = file_data["sha"]
        current_content = base64.b64decode(file_data["content"]).decode('utf-8')
    
    # 2. Check if content is actually different
    if new_content == current_content:
        print(f"Skipping {file_path}: Content is identical to current version on branch.")
        return False

    # 3. Commit
    commit_url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
    data = {
        "message": f"docs: auto-update {file_path}",
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": current_sha, # Will be None for new files, valid for existing
        "branch": target_branch
    }
    
    try:
        github_put(commit_url, data)
        print(f"Committed: {file_path}")
        return True
    except Exception as e:
        print(f"Failed to commit {file_path}: {e}")
        return False

def raise_pull_request(repo_name, head_branch, modified_files, original_pr_number):
    new_branch_name = f"doc-update/pr-{original_pr_number}-auto"
    
    # 1. Create Branch (or ensure it exists)
    # We branch off the PR's head branch so the doc updates are based on the new code
    source_sha = get_ref_sha(repo_name, head_branch)
    create_branch(repo_name, new_branch_name, source_sha)

    # 2. Commit Changes
    commits_made = 0
    for file_data in modified_files:
        success = commit_file(repo_name, file_data["path"], file_data["new_content"], new_branch_name)
        if success:
            commits_made += 1

    # 3. Create Pull Request (Only if we made changes)
    if commits_made == 0:
        print("No new commits made (files might be identical). Skipping PR creation.")
        return

    pr_url = f"https://api.github.com/repos/{repo_name}/pulls"
    pr_data = {
        "title": f"[Auto-Generated] Documentation updates for PR #{original_pr_number}",
        "body": f"Automated documentation updates triggered by changes in PR #{original_pr_number}.",
        "head": new_branch_name,
        "base": head_branch # Target the feature branch (Stacked PR)
    }
    
    try:
        pr_response = github_post(pr_url, pr_data)
        print(f"\nSuccess! Pull Request created: {pr_response['html_url']}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print("PR already exists or validation failed.")
            print(e.response.json())
        else:
            raise e

# --- Main Orchestrator ---

def main(repo_name, pr_number):
    print(f"--- Starting Flow for {repo_name} PR #{pr_number} ---")

    # Step 1: Fetch Data
    print("\nStep 1: Fetching Data...")
    pr_info = get_pr_data(repo_name, pr_number)
    docs_map = get_all_adoc_files(repo_name, pr_info["head_branch"])
    
    if not docs_map:
        print("No AsciiDoc files found.")
        return

    # Step 2: Analysis
    print("\nStep 2: Analyzing Gaps...")
    proposed_changes = analyze_diff_in_batches(pr_info["diff"], docs_map)
    
    if not proposed_changes:
        print("No gaps found.")
        return

    # Step 3: Refactoring (Generate Content only)
    print("\nStep 3: Refactoring Files...")
    modified_files = process_refactoring(proposed_changes, repo_name, pr_info["head_branch"], pr_info["diff"])
    
    # Step 4: Raise PR (Commit + PR)
    print("\nStep 4: Creating PR...")
    raise_pull_request(repo_name, pr_info["head_branch"], modified_files, pr_number)

if __name__ == "__main__":
    REPO = "owner/repo-name"
    PR_NUM = 12
    
    if not GITHUB_TOKEN or not OPENAI_API_KEY:
        print("Set GITHUB_TOKEN and OPENAI_API_KEY env vars.")
    else:
        main(REPO, PR_NUM)
