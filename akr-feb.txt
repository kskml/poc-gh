import os
import json
import base64
import requests
from openai import OpenAI

# --- Configuration & Clients ---
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github+json",
    "X-GitHub-Api-Version": "2022-11-28"
}

openai_client = OpenAI(api_key=OPENAI_API_KEY)

# --- Helper: Generic GitHub REST Calls ---

def github_get(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 404:
        return None
    response.raise_for_status()
    return response.json()

def github_put(url, data):
    response = requests.put(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

def github_post(url, data):
    response = requests.post(url, headers=HEADERS, json=data)
    response.raise_for_status()
    return response.json()

# --- Step 1: Data Ingestion ---

def get_pr_data(repo_name, pr_number):
    url = f"https://api.github.com/repos/{repo_name}/pulls/{pr_number}"
    pr_data = github_get(url)
    
    diff_response = requests.get(pr_data["diff_url"])
    diff_response.raise_for_status()
    
    return {
        "diff": diff_response.text,
        "base_branch": pr_data["base"]["ref"],
        "head_branch": pr_data["head"]["ref"],
        "head_sha": pr_data["head"]["sha"]
    }

def get_all_adoc_files(repo_name, branch_ref):
    tree_url = f"https://api.github.com/repos/{repo_name}/git/trees/{branch_ref}?recursive=1"
    tree_data = github_get(tree_url)
    
    docs_map = {}
    if not tree_data: return docs_map

    for item in tree_data.get("tree", []):
        if item["type"] == "blob" and item["path"].endswith(".adoc"):
            content_url = f"https://api.github.com/repos/{repo_name}/contents/{item['path']}?ref={branch_ref}"
            try:
                file_data = github_get(content_url)
                if file_data:
                    decoded_content = base64.b64decode(file_data["content"]).decode('utf-8')
                    docs_map[item["path"]] = decoded_content
            except Exception as e:
                print(f"Warning: Could not read {item['path']}: {e}")
                
    return docs_map

# --- Step 2: LLM Analysis ---

def chunk_dict(data, chunk_size=3):
    items = list(data.items())
    for i in range(0, len(items), chunk_size):
        yield dict(items[i:i + chunk_size])

def analyze_diff_in_batches(code_diff, docs_map, batch_size=3):
    all_proposed_changes = []
    batches = list(chunk_dict(docs_map, batch_size))
    
    print(f"Analyzing {len(docs_map)} files in {len(batches)} batches...")

    for i, batch in enumerate(batches):
        batch_context = "\n\n".join([f"File: {path}\nContent:\n{content}" for path, content in batch.items()])

        prompt = f"""
        You are a Technical Documentation Expert.
        Analyze the 'Code Diff' against the 'Documentation Files'.
        
        Code Diff:
        {code_diff}

        Documentation Files:
        {batch_context}

        Task:
        Identify gaps where the Code Diff contradicts or is not reflected in the documentation.
        Return a JSON object with a key "updates" containing a list of objects.
        Each object must have:
        1. "filename": The exact file path.
        2. "proposed_changes": A precise description of what needs to be updated.
        
        If no updates needed, return {{"updates": []}}.
        """

        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            if "updates" in result:
                valid_updates = [u for u in result["updates"] if u.get("filename") and u.get("proposed_changes")]
                all_proposed_changes.extend(valid_updates)
                
        except Exception as e:
            print(f"Error in batch {i+1}: {e}")

    return all_proposed_changes

# --- Step 3: Refactoring (Fixed) ---

def clean_adoc_content(content):
    """
    Removes common LLM artifacts like Markdown code blocks.
    """
    content = content.strip()
    # Remove markdown code block wrappers if LLM added them
    if content.startswith("```asciidoc"):
        content = content[len("```asciidoc"):]
    if content.startswith("```"):
        content = content[len("```"):]
    if content.endswith("```"):
        content = content[:-len("```")]
    return content.strip()

def refactor_identified_file(original_content, proposed_changes, code_diff):
    """
    Instructs LLM to apply minimal updates to the existing content.
    """
    prompt = f"""
    You are an AsciiDoc specialist tasked with updating documentation.
    
    # INSTRUCTIONS
    1.  Apply the 'Proposed Changes' to the 'Original Document'.
    2.  **CRITICAL**: Preserve the existing structure, formatting, and content that is not affected by the changes. Do NOT rewrite the entire document from scratch. Only modify the necessary sections.
    3.  Output MUST be valid AsciiDoc syntax.
    4.  Do NOT use Markdown formatting (no ``` blocks). Return raw text only.
    
    # Original Document
    {original_content}
    
    # Proposed Changes
    {proposed_changes}
    
    # Context (Code Diff)
    {code_diff}
    
    # Updated Document (Raw AsciiDoc only)
    """

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1 # Lower temperature for more deterministic/faithful copying
        )
        
        # Clean up potential markdown artifacts
        return clean_adoc_content(response.choices[0].message.content)
        
    except Exception as e:
        print(f"LLM Refactoring error: {e}")
        return None

def process_refactoring(proposed_changes, repo_name, head_branch, code_diff):
    refactored_files = []
    
    print(f"\nRefactoring {len(proposed_changes)} identified files...")

    for change in proposed_changes:
        filename = change['filename']
        instruction = change['proposed_changes']
        
        print(f"Processing: {filename}")
        
        try:
            # Fetch content from source branch
            url = f"https://api.github.com/repos/{repo_name}/contents/{filename}?ref={head_branch}"
            file_data = github_get(url)
            if not file_data:
                print(f"File {filename} not found, skipping.")
                continue
                
            original_content = base64.b64decode(file_data["content"]).decode('utf-8')
            
            # Get Refactored Content
            new_content = refactor_identified_file(original_content, instruction, code_diff)
            
            if new_content:
                refactored_files.append({
                    "path": filename,
                    "new_content": new_content
                })
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            
    return refactored_files

# --- Step 4: GitHub PR Creation ---

def get_ref_sha(repo_name, ref):
    url = f"https://api.github.com/repos/{repo_name}/git/ref/heads/{ref}"
    data = github_get(url)
    return data["object"]["sha"]

def create_branch(repo_name, new_branch_name, source_sha):
    url = f"https://api.github.com/repos/{repo_name}/git/refs"
    data = {"ref": f"refs/heads/{new_branch_name}", "sha": source_sha}
    try:
        github_post(url, data)
        print(f"Created new branch: {new_branch_name}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print(f"Branch '{new_branch_name}' already exists. Reusing it.")
        else:
            raise e

def commit_file(repo_name, file_path, new_content, target_branch):
    # Get SHA from the target branch (doc branch)
    url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}?ref={target_branch}"
    file_data = github_get(url)
    
    current_content = ""
    current_sha = None
    
    if file_data and "content" in file_data:
        current_sha = file_data["sha"]
        current_content = base64.b64decode(file_data["content"]).decode('utf-8')
    
    # Skip if identical
    if new_content == current_content:
        print(f"Skipping {file_path}: No changes detected.")
        return False

    commit_url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
    data = {
        "message": f"docs: update {file_path}",
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": current_sha,
        "branch": target_branch
    }
    
    try:
        github_put(commit_url, data)
        print(f"Committed: {file_path}")
        return True
    except Exception as e:
        print(f"Failed to commit {file_path}: {e}")
        return False

def raise_pull_request(repo_name, head_branch, modified_files, original_pr_number):
    new_branch_name = f"doc-update/pr-{original_pr_number}-auto"
    
    # 1. Create/Ensure Branch exists
    source_sha = get_ref_sha(repo_name, head_branch)
    create_branch(repo_name, new_branch_name, source_sha)

    # 2. Commit Changes
    commits_made = 0
    for file_data in modified_files:
        success = commit_file(repo_name, file_data["path"], file_data["new_content"], new_branch_name)
        if success:
            commits_made += 1

    # 3. Create PR
    if commits_made == 0:
        print("No new commits to raise PR for.")
        return

    pr_url = f"https://api.github.com/repos/{repo_name}/pulls"
    pr_data = {
        "title": f"[Auto-Generated] Documentation updates for PR #{original_pr_number}",
        "body": f"Automated updates based on code changes in PR #{original_pr_number}.",
        "head": new_branch_name,
        "base": head_branch
    }
    
    try:
        pr_response = github_post(pr_url, pr_data)
        print(f"\nSuccess! Pull Request created: {pr_response['html_url']}")
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 422:
            print("PR already exists or validation failed.")
        else:
            raise e

# --- Main Orchestrator ---

def main(repo_name, pr_number):
    print(f"--- Starting Flow for {repo_name} PR #{pr_number} ---")

    print("\nStep 1: Fetching Data...")
    pr_info = get_pr_data(repo_name, pr_number)
    docs_map = get_all_adoc_files(repo_name, pr_info["head_branch"])
    
    if not docs_map:
        print("No AsciiDoc files found.")
        return

    print("\nStep 2: Analyzing Gaps...")
    proposed_changes = analyze_diff_in_batches(pr_info["diff"], docs_map)
    
    if not proposed_changes:
        print("No gaps found.")
        return

    print("\nStep 3: Refactoring Files...")
    modified_files = process_refactoring(proposed_changes, repo_name, pr_info["head_branch"], pr_info["diff"])
    
    print("\nStep 4: Creating PR...")
    raise_pull_request(repo_name, pr_info["head_branch"], modified_files, pr_number)

if __name__ == "__main__":
    REPO = "owner/repo-name"
    PR_NUM = 12
    
    if not GITHUB_TOKEN or not OPENAI_API_KEY:
        print("Set GITHUB_TOKEN and OPENAI_API_KEY env vars.")
    else:
        main(REPO, PR_NUM)
