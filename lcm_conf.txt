import os
import requests
import re
from dotenv import load_dotenv
from openai import AzureOpenAI
from markdownify import markdownify as md

# Load environment variables
load_dotenv()

# --- CONFIGURATION ---
# Confluence
CONFLUENCE_BASE_URL = os.getenv("CONFLUENCE_BASE_URL")
CONFLUENCE_EMAIL = os.getenv("CONFLUENCE_EMAIL")
CONFLUENCE_API_TOKEN = os.getenv("CONFLUENCE_API_TOKEN")
PAGE_ID = os.getenv("PAGE_ID")

# GitHub
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GITHUB_REPO_OWNER = os.getenv("GITHUB_REPO_OWNER")
GITHUB_REPO_NAME = os.getenv("GITHUB_REPO_NAME")
GITHUB_BASE_TAG = os.getenv("GITHUB_BASE_TAG")   # e.g., "v1.0.0"
GITHUB_HEAD_TAG = os.getenv("GITHUB_HEAD_TAG")   # e.g., "v1.1.0"

# Azure
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
# ----------------------

def clean_confluence_html(html_content):
    """
    Replaces complex Confluence macros (Draw.io, PlantUML) with clean text.
    """
    print("   Cleaning Draw.io and PlantUML macros from HTML...")
    
    # 1. Handle Draw.io (Extract Name)
    # Regex matches the macro structure and finds the <ac:parameter> for "diagramName"
    pattern_drawio = re.compile(
        r'<ac:structured-macro ac:name="drawio"[^>]*>.*?' + 
        r'<ac:parameter ac:name="diagramName">(.*?)</ac:parameter>' + 
        r'.*?</ac:structured-macro>', 
        re.DOTALL
    )

    def replace_drawio(match):
        name = match.group(1).strip()
        return f"\n\n**[Draw.io Diagram: {name}]**\n\n"

    cleaned_html = pattern_drawio.sub(replace_drawio, html_content)

    # 2. Handle PlantUML (Extract Title if present)
    # PlantUML macros often have a "title" parameter.
    # We check for that first to make the placeholder specific.
    pattern_plantuml_title = re.compile(
        r'<ac:structured-macro ac:name="plantUML"[^>]*>.*?' + 
        r'<ac:parameter ac:name="title">(.*?)</ac:parameter>' + 
        r'.*?</ac:structured-macro>', 
        re.DOTALL
    )

    def replace_plantuml_title(match):
        title = match.group(1).strip()
        return f"\n\n**[PlantUML Diagram: {title}]**\n\n"

    cleaned_html = pattern_plantuml_title.sub(replace_plantuml_title, cleaned_html)

    # 3. Fallback for PlantUML (No title found, or different structure)
    # This matches any remaining PlantUML macro and replaces it with a generic marker
    pattern_plantuml_generic = re.compile(
        r'<ac:structured-macro ac:name="plantUML"[^>]*>.*?</ac:structured-macro>', 
        re.DOTALL
    )
    cleaned_html = pattern_plantuml_generic.sub("\n\n**[PlantUML Diagram]**\n\n", cleaned_html)

    # 4. General Cleanup (Remove any other structured macros preventing noise)
    # This is a catch-all to ensure no garbage like 'true{...}falsetext' appears
    fallback_pattern = re.compile(r'<ac:structured-macro[^>]*>.*?</ac:structured-macro>', re.DOTALL)
    cleaned_html = fallback_pattern.sub("\n\n**[Confluence Macro]**\n\n", cleaned_html)

    return cleaned_html

def get_confluence_content(base_url, email, token, page_id):
    """Fetches documentation, cleans HTML artifacts, and converts to Markdown."""
    auth = (email, token)
    url = f"{base_url}/rest/api/content/{page_id}?expand=body.storage"
    print(f"Fetching Confluence page ID: {page_id}...")
    response = requests.get(url, auth=auth)
    if response.status_code != 200:
        raise Exception(f"Confluence API Error: {response.status_code}")
    
    raw_html = response.json()['body']['storage']['value']
    
    # Clean the HTML before Markdown conversion
    clean_html = clean_confluence_html(raw_html)
    
    # Convert to Markdown
    return md(clean_html)

def find_top_header_level(markdown_text):
    """
    Scans the text to find the highest level header present (e.g., 1 for #, 2 for ##).
    """
    lines = markdown_text.split('\n')
    min_level = 100 # Start high
    
    for line in lines:
        if line.strip().startswith('#'):
            stripped = line.strip().split()[0]
            level = len(stripped)
            if level < min_level:
                min_level = level
                
    # If no headers found, default to 1
    return 1 if min_level == 100 else min_level

def chunk_confluence_hierarchical(markdown_text):
    """
    Splits markdown into chunks based on detected top header level.
    """
    print("Detecting top header level...")
    split_level = find_top_header_level(markdown_text)
    print(f"Splitting Confluence content (Auto-detected Level {split_level} as main section header)...")
    
    lines = markdown_text.split('\n')
    chunks = []
    
    # Initialize as None
    current_header = None
    current_content = []
    
    for line in lines:
        if line.strip().startswith('#'):
            stripped_line = line.strip()
            header_marker = stripped_line.split()[0] 
            header_level = len(header_marker)
            
            # Only split if we hit the target level
            if header_level <= split_level:
                # Save previous chunk
                if current_content:
                    chunks.append({
                        'header': current_header if current_header else 'Preamble',
                        'content': "\n".join(current_content)
                    })
                
                # Start new chunk
                current_header = stripped_line
                current_content = []
            else:
                # It is a subheader, keep it as content
                current_content.append(line)
        else:
            current_content.append(line)
            
    # Save last chunk
    if current_content:
        chunks.append({
            'header': current_header if current_header else 'Preamble',
            'content': "\n".join(current_content)
        })
        
    print(f"‚úÖ Created {len(chunks)} sections.")
    return chunks

def get_full_github_diff(owner, repo, base_tag, head_tag, token):
    """Fetches the complete diff."""
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }
    url = f"https://api.github.com/repos/{owner}/{repo}/compare/{base_tag}...{head_tag}"
    print(f"Fetching GitHub Diff: {base_tag} -> {head_tag}...")
    
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"GitHub API Error: {response.status_code} - {response.text}")
    
    diff_content = response.json().get('diff', '')
    print(f"‚úÖ Diff retrieved (Length: {len(diff_content)} chars).")
    return diff_content

def analyze_single_section(client, deployment_name, section_header, section_content, full_diff):
    """
    Invokes LLM for a SINGLE section of documentation against the FULL diff.
    Uses strict variable injection to ensure correct reporting.
    """
    
    system_prompt = """
    You are an ASPICE SWE.2 Auditor.
    You are analyzing ONE specific section of documentation against a Software Release Code Diff.
    
    **Instructions:**
    1. Read the "Confluence Section".
    2. Read the "GitHub Code Diff".
    3. Determine if the changes in the Diff affect this specific Section.
    
    **Output Rules:**
    - If the code diff is FULLY ALIGNED with the section and no documentation updates are needed, return exactly: `NO_IMPACT`.
    - If there IS a gap, return ONLY a Markdown table row (do not include table headers).
    
    **Severity Definitions:**
    - üî¥ **NON-CONFORMANT:** Code contradicts doc.
    - üü† **GAP IDENTIFIED:** Code adds new feature.
    - üî¥ **NON-CONFORMANT / GAP:** Complex change.
    - üü° **INCONSISTENCY:** Minor mismatch.
    
    **CRITICAL OUTPUT REQUIREMENT:**
    - The "Confluence Section" column in your output table MUST contain the value enclosed in <SECTION_TITLE> tags exactly.
    
    **Row Format:**
    | ID | Severity | Confluence Section | Impact Scope | SWE.2 Observation (Functional) | Corrective Action |
    | 1 | üî¥ **NON-CONFORMANT** | <SECTION_TITLE> | UI/Logic/Data/API | [Observation] | [Action] |
    """

    # Inject the variable explicitly to prevent LLM confusion with preamble content
    user_prompt = f"""
    <SECTION_TITLE>{section_header}</SECTION_TITLE>
    
    CONFLUENCE CONTENT TO ANALYZE:
    {section_content}

    ---
    
    GITHUB CODE DIFF (Release Changes):
    {full_diff}
    """
    
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error analyzing section '{section_header}': {e}")
        return "NO_IMPACT"

def main():
    # 1. Initialize Clients
    try:
        client = AzureOpenAI(
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT
        )
    except Exception as e:
        print(f"‚ùå Initialization Error: {e}")
        return

    if not all([CONFLUENCE_BASE_URL, CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN, PAGE_ID, 
                GITHUB_TOKEN, GITHUB_REPO_OWNER, GITHUB_REPO_NAME, GITHUB_BASE_TAG, GITHUB_HEAD_TAG]):
        print("‚ùå Error: Missing configuration.")
        return

    try:
        # 2. Fetch Docs
        doc_text = get_confluence_content(CONFLUENCE_BASE_URL, CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN, PAGE_ID)
        
        # 3. Chunk Docs (Auto-Detects Header Level)
        sections = chunk_confluence_hierarchical(doc_text)
        
        # 4. Fetch GitHub Diff (Full Context)
        full_diff = get_full_github_diff(GITHUB_REPO_OWNER, GITHUB_REPO_NAME, GITHUB_BASE_TAG, GITHUB_HEAD_TAG, GITHUB_TOKEN)
        
        if not full_diff:
            print("‚ö†Ô∏è  No Diff found to analyze.")
            return

        # 5. Iterative Analysis
        print(f"üîÑ Starting Analysis across {len(sections)} major sections...")
        accumulated_rows = []
        
        for idx, section in enumerate(sections):
            print(f"   Analyzing Section {idx+1}/{len(sections)}: {section['header']}")
            
            result = analyze_single_section(
                client, 
                AZURE_OPENAI_DEPLOYMENT, 
                section['header'], 
                section['content'], 
                full_diff
            )
            
            # 6. Filter Logic: Skip NO_IMPACT
            if result and result != "NO_IMPACT":
                accumulated_rows.append(result)
                print(f"      ‚Üí Impact Found.")
            else:
                print(f"      ‚Üí No Impact. Skipping.")

        # 7. Generate Final Markdown Report
        if not accumulated_rows:
            print("‚úÖ No gaps found across all sections. Report not generated.")
        else:
            with open("Clean_Plantuml_Report.md", "w", encoding="utf-8") as f:
                f.write("# ASPICE SWE.2 Sectional Impact Analysis\n\n")
                f.write(f"**Release Comparison:** `{GITHUB_BASE_TAG}` ‚Üí `{GITHUB_HEAD_TAG}`\n\n")
                f.write("---\n\n")
                f.write("## Assessment Criteria & Definitions\n\n")
                f.write("| Option | Definition |\n")
                f.write("| :--- | :--- |\n")
                f.write("| üî¥ **NON-CONFORMANT** | Code behavior contradicts the existing Functional Specification. |\n")
                f.write("| üü† **GAP IDENTIFIED** | Code implements new functionality that is missing from the Specification. |\n")
                f.write("| üî¥ **NON-CONFORMANT / GAP** | A complex change that **both** violates the existing specification AND introduces new undocumented behavior. |\n")
                f.write("| üü° **INCONSISTENCY** | Minor discrepancies in parameter definitions or descriptions. |\n")
                f.write("\n---\n\n")
                f.write("## Detailed Functional Analysis\n\n")
                
                f.write("| ID | Severity | Confluence Section | Impact Scope | SWE.2 Observation (Functional) | Corrective Action |\n")
                f.write("| :--- | :--- | :--- | :--- | :--- | :--- |\n")
                
                for row in accumulated_rows:
                    clean_row = row.replace("\n", "<br/>")
                    f.write(clean_row + "\n")
            
            print(f"‚úÖ Report generated: Clean_Plantuml_Report.md")

    except Exception as e:
        print(f"‚ùå Execution Error: {e}")

if __name__ == "__main__":
    main()
