import os
import requests
import re
import argparse
import json
from dotenv import load_dotenv
from openai import AzureOpenAI
from markdownify import markdownify as md
from bs4 import BeautifulSoup
from pathlib import Path

# Load environment variables
load_dotenv()

def clean_confluence_html(html_content):
    """Uses BeautifulSoup to safely remove macros."""
    print("   Cleaning macros with BeautifulSoup...")
    soup = BeautifulSoup(html_content, 'html.parser')
    macros = soup.find_all('ac:structured-macro')
    
    if not macros:
        return str(soup)
        
    for macro in macros:
        macro_name = macro.get('ac:name', '').lower()
        if 'plantuml' in macro_name or 'drawio' in macro_name or 'draw.io' in macro_name:
            diag_name = "Unknown"
            params = macro.find_all('ac:parameter')
            for param in params:
                param_name = param.get('ac:name', '').lower()
                if param_name in ['diagramname', 'title', 'filename', 'name']:
                    diag_name = param.get_text(strip=True)
                    break
            
            if 'plantuml' in macro_name:
                placeholder = f"\n\n<<< PLANTUML DIAGRAM: {diag_name} >>>\n\n"
            else:
                placeholder = f"\n\n**[Draw.io Diagram: {diag_name}]**\n\n"
            
            macro.replace_with(placeholder)
    
    return str(soup)

def get_confluence_content(base_url, email, token, page_id):
    auth = (email, token)
    url = f"{base_url}/rest/api/content/{page_id}?expand=body.storage"
    print(f"Fetching Confluence page ID: {page_id}...")
    response = requests.get(url, auth=auth)
    if response.status_code != 200:
        raise Exception(f"Confluence API Error: {response.status_code}")
    raw_html = response.json()['body']['storage']['value']
    clean_html = clean_confluence_html(raw_html)
    return md(clean_html)

def find_local_diagram(diagram_name, local_path):
    if not local_path or not os.path.exists(local_path):
        return None
    target_name_normalized = diagram_name.replace(" ", "_").replace("-", "_").lower()
    extensions = ['.puml', '.pu', '.plantuml']
    path_obj = Path(local_path)
    
    for ext in extensions:
        potential_file = path_obj / f"{target_name_normalized}{ext}"
        if potential_file.exists():
            return potential_file.read_text(encoding='utf-8')
        for file in path_obj.glob(f"*{ext}"):
            file_stem = file.stem.lower().replace(" ", "_").replace("-", "_")
            if file_stem == target_name_normalized:
                print(f"      üóÇÔ∏è  Found matching diagram: {file.name}")
                return file.read_text(encoding='utf-8')
    return None

def chunk_confluence_hierarchical(markdown_text, split_level):
    print(f"Splitting Confluence content at Level {split_level}...")
    lines = markdown_text.split('\n')
    chunks = []
    current_header = None
    current_content = []
    
    for line in lines:
        if line.strip().startswith('#'):
            stripped_line = line.strip()
            header_marker = stripped_line.split()[0] 
            header_level = len(header_marker)
            if header_level <= split_level:
                if current_header is not None:
                    chunks.append({
                        'header': current_header,
                        'content': "\n".join(current_content)
                    })
                current_header = stripped_line
                current_content = []
                current_content.append(line)
            else:
                current_content.append(line)
        else:
            current_content.append(line)
            
    if current_header is not None:
        chunks.append({
            'header': current_header,
            'content': "\n".join(current_content)
        })
    print(f"‚úÖ Created {len(chunks)} sections.")
    return chunks

def get_full_github_diff(owner, repo, base_tag, head_tag, token):
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }
    url = f"https://api.github.com/repos/{owner}/{repo}/compare/{base_tag}...{head_tag}"
    print(f"Fetching GitHub Diff: {base_tag} -> {head_tag}...")
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"GitHub API Error: {response.status_code} - {response.text}")
    diff_content = response.json().get('diff', '')
    print(f"‚úÖ Diff retrieved.")
    return diff_content

def analyze_single_section(client, deployment_name, section_header, section_content, full_diff, local_diag_path, exclude_list):
    """
    Invokes LLM with JSON Output Strategy.
    LLM returns pure data (JSON), Script builds the HTML Table.
    """
    
    # 1. Check Exclusions
    for exclude_term in exclude_list:
        if exclude_term.lower() in section_header.lower():
            return "NO_IMPACT"
    
    # 2. Regex for Diagrams
    pattern = re.compile(r'<<<\s*(PLANTUML|DRAWIO|DRAW)\s*DIAGRAM(.*?)\s*>>>', re.IGNORECASE)
    matches = pattern.findall(section_content)
    
    diagram_contexts = []
    for match in matches:
        diag_type, diag_name = match
        diag_name = diag_name.strip(" :")
        if not diag_name:
            diag_name = "Unknown"
        if "plantuml" in diag_type.lower():
            diag_content = find_local_diagram(diag_name, local_diag_path)
            if diag_content:
                diagram_contexts.append(f"\n--- DIAGRAM CONTEXT: {diag_name} ---\n{diag_content}\n")
    
    prompt_diagrams = "\n".join(diagram_contexts) if diagram_contexts else "No local diagrams found for this section."
    
    system_prompt = """
    You are an ASPICE SWE.2 Auditor.
    
    **Instructions:**
    1. Check if Code Diff affects Text OR Diagram Logic.
    2. If aligned, return JSON: { "status": "no_impact" }.
    3. If gap found, return a JSON object with exact keys below.
    
    **JSON FORMAT (Strict):**
    {
      "status": "gap_found",
      "severity": "NON-CONFORMANT" or "GAP IDENTIFIED" or "INCONSISTENCY",
      "section_header": "The exact section title",
      "observation": "Short functional observation",
      "corrective_action_text": "Short functional instruction (e.g., 'Add state X').",
      "updated_diagram_code": "FULL CODE BLOCK here OR null (if no diagram change needed)"
    }
    
    **Diagram Rules:**
    - Only include 'updated_diagram_code' key if a diagram needs modification.
    - Use standard Markdown code block syntax (```plantuml ... ``` ).
    - Do NOT include extra text in the code block key, only the code.
    """

    user_prompt = f"""
    <SECTION_TITLE>{section_header}</SECTION_TITLE>
    
    CONFLUENCE TEXT CONTENT:
    {section_content}

    --- 
    
    LOCAL DIAGRAMS (PlantUML Logic):
    {prompt_diagrams}

    ---
    
    GITHUB CODE DIFF:
    {full_diff}
    """
    
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error analyzing section '{section_header}': {e}")
        return "NO_IMPACT"

def build_html_table(gap_data):
    """
    Generates a robust HTML Table string from a list of JSON dictionaries.
    This guarantees the table structure is never broken.
    """
    if not gap_data:
        return "<p>No gaps found.</p>"

    html_parts = []
    html_parts.append("<table class='wrapped'><tbody>")
    
    # Define Headers (CSS classes optional, kept simple for now)
    headers = ["ID", "Severity", "Confluence Section", "Impact Scope", "SWE.2 Observation", "Corrective Action", "Updated PlantUML Diagram"]
    
    # Header Row
    header_html = "<thead><tr>"
    for h in headers:
        header_html += f"<th>{h}</th>"
    header_html += "</tr></thead>"
    html_parts.append(header_html)

    # Data Rows
    for idx, item in enumerate(gap_data):
        # Get values
        severity = item.get("severity", "").upper()
        section = item.get("section_header", "")
        scope = item.get("scope", "Logic/Data/API") # Fallback if LLM misses, though we requested it
        obs = item.get("observation", "")
        action_text = item.get("corrective_action_text", "")
        diag_code = item.get("updated_diagram_code")
        
        # HTML Escaping
        # Replace | with &#124; to prevent table breaks
        section_escaped = section.replace("|", "&#124;")
        obs_escaped = obs.replace("|", "&#124;")
        action_escaped = action_text.replace("|", "&#124;")
        diag_escaped = diag_code.replace("|", "&#124;") if diag_code else ""
        
        # Format Diagram Cell
        diag_cell_content = ""
        if diag_code:
            # Replace newlines in diagram with <br/> so it fits in cell
            diag_code_clean = diag_code.replace("\n", "<br/>")
            diag_cell_content = f"<pre><code class='language-plantuml'>{diag_code_clean}</code></pre>"
        else:
            diag_cell_content = "<em>N/A</em>"
            
        row_html = f"<tr>"
        row_html += f"<td>{idx + 1}</td>"
        row_html += f"<td>{severity}</td>"
        row_html += f"<td>{section_escaped}</td>"
        row_html += f"<td>{scope}</td>"
        row_html += f"<td>{obs_escaped}</td>"
        row_html += f"<td>{action_escaped}</td>"
        row_html += f"<td>{diag_cell_content}</td>"
        row_html += "</tr>"
        html_parts.append(row_html)
        
    html_parts.append("</tbody></table>")
    return "".join(html_parts)

def main():
    parser = argparse.ArgumentParser(description="ASPICE SWE.2 Confluence vs Code Analyzer (JSON Mode)")
    
    parser.add_argument("--base-tag", required=True, help="Base Git Tag")
    parser.add_argument("--head-tag", required=True, help="Head Git Tag")
    parser.add_argument("--page-id", required=True, help="Confluence Page ID")
    parser.add_argument("--module-path", required=True, help="Local Path to Diagrams")
    parser.add_argument("--split-level", type=int, default=3, help="Split Level")
    parser.add_argument("--exclude-sections", default="", help="Comma separated keywords to skip")
    parser.add_argument("--repo-owner", default=os.getenv("GITHUB_REPO_OWNER"), help="GitHub Repo Owner")
    parser.add_argument("--repo-name", default=os.getenv("GITHUB_REPO_NAME"), help="GitHub Repo Name")
    
    args = parser.parse_args()
    exclude_list = [x.strip().lower() for x in args.exclude_sections.split(",")] if args.exclude_sections else []

    try:
        client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
    except Exception as e:
        print(f"‚ùå Initialization Error: {e}")
        return

    confl_base = os.getenv("CONFLUENCE_BASE_URL")
    confl_email = os.getenv("CONFLUENCE_EMAIL")
    confl_token = os.getenv("CONFLUENCE_API_TOKEN")
    github_token = os.getenv("GITHUB_TOKEN")
    deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")

    if not all([confl_base, confl_email, confl_token, github_token, deployment_name]):
        print("‚ùå Error: Missing Secrets in Environment Variables.")
        return

    try:
        doc_text = get_confluence_content(confl_base, confl_email, confl_token, args.page_id)
        sections = chunk_confluence_hierarchical(doc_text, args.split_level)
        
        full_diff = get_full_github_diff(
            args.repo_owner, args.repo_name, 
            args.base_tag, args.head_tag, 
            github_token
        )
        
        if not full_diff:
            print("‚ö†Ô∏è  No Diff found.")
            return

        print(f"üîÑ Starting Analysis across {len(sections)} sections...")
        accumulated_gaps = []
        
        for idx, section in enumerate(sections):
            print(f"   Analyzing Section {idx+1}/{len(sections)}: {section['header']}")
            
            result_str = analyze_single_section(
                client, 
                deployment_name, 
                section['header'], 
                section['content'], 
                full_diff,
                args.module_path,
                exclude_list
            )
            
            # Parse JSON
            if result_str and result_str != "NO_IMPACT":
                try:
                    # Use regex to extract JSON in case LLM adds backticks
                    match = re.search(r'\{.*\}', result_str, re.DOTALL)
                    if match:
                        json_str = match.group(0)
                        data = json.loads(json_str)
                        
                        # Only add if LLM status is not no_impact
                        if data.get("status") != "no_impact":
                            accumulated_gaps.append(data)
                            print(f"      ‚Üí Gap Found.")
                        else:
                            print(f"      ‚úÖ Aligned.")
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è  Failed to parse JSON: {e}")
                else:
                    print(f"      ‚úÖ No Impact.")

        # Generate Report
        if not accumulated_gaps:
            print("‚úÖ No gaps found.")
        else:
            with open("Arg_Analyzer_Report.md", "w", encoding="utf-8") as f:
                f.write("# ASPICE SWE.2 Analysis\n\n")
                f.write(f"**Release:** `{args.base_tag}` ‚Üí `{args.head_tag}`\n\n")
                f.write("## Assessment Criteria\n\n")
                f.write("| Option | Definition |\n| :--- | :--- |\n")
                f.write("| üî¥ **NON-CONFORMANT** | Code contradicts doc or diagram. |\n")
                f.write("| üü† **GAP IDENTIFIED** | Code adds missing feature. |\n")
                f.write("| üî¥ **NON-CONFORMANT / GAP** | Complex change. |\n")
                f.write("| üü° **INCONSISTENCY** | Minor mismatch. |\n\n")
                f.write("## Analysis\n\n")
                
                # Build Table
                table_html = build_html_table(accumulated_gaps)
                f.write(table_html)
            
            print(f"‚úÖ Report generated (HTML Table via JSON).")

    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()
