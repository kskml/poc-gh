import os
import json
import hashlib
import tiktoken
from openai import AzureOpenAI
from dotenv import load_dotenv
import datetime

# ==========================================
# 1. SETUP & CONFIGURATION
# ==========================================

def get_azure_client():
    """Initializes and returns the Azure OpenAI client."""
    load_dotenv()
    
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    return client

# ==========================================
# 2. FILE SYSTEM OPERATIONS
# ==========================================

def read_source_code_files(source_dir, extensions=[".py", ".js", ".java", ".cs", ".ts"]):
    """
    Recursively reads all files with given extensions from the directory.
    Returns:
        1. code_files_dict: {filepath: content}
        2. raw_concat: Raw string of all content (for hashing)
    """
    code_files = {}
    raw_content_for_hash = ""
    
    if not os.path.exists(source_dir):
        raise FileNotFoundError(f"Source directory not found: {source_dir}")

    for root, dirs, files in os.walk(source_dir):
        # Skip non-essential directories
        dirs[:] = [d for d in dirs if d not in ['venv', '__pycache__', 'node_modules', '.git', 'env', 'build', 'dist']]
        
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        rel_path = os.path.relpath(file_path, source_dir)
                        code_files[rel_path] = content
                        raw_content_for_hash += f"FILE:{rel_path}\n{content}\n"
                except Exception as e:
                    print(f"Warning: Could not read {file_path}: {e}")
                    
    return code_files, raw_content_for_hash

def read_hld_file(hld_path):
    """Reads the HLD document content."""
    if not os.path.exists(hld_path):
        raise FileNotFoundError(f"HLD file not found: {hld_path}")
    
    with open(hld_path, 'r', encoding='utf-8') as f:
        return f.read()

# ==========================================
# 3. TOON IMPLEMENTATION
# ==========================================

def dict_to_toon(data):
    """
    Converts a dictionary to TOON (Table Oriented Object Notation).
    Format: 'Key | Value'
    """
    if not data:
        return "No data provided"
    
    rows = []
    rows.append("File Path | Content")
    
    for key, value in data.items():
        # We keep newlines as escaped \\n for the table structure visibility
        clean_val = str(value).replace("\n", "\\n") 
        rows.append(f"{key} | {clean_val}")
            
    return "\n".join(rows)

# ==========================================
# 4. TOKEN COUNTING UTILITIES
# ==========================================

def count_tokens_tiktoken(text, model="gpt-4"):
    """
    Used ONLY for Baseline Simulation.
    Actual usage comes from API response.
    """
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# ==========================================
# 5. LLM CACHING STRATEGY
# ==========================================

def generate_cache_key(source_raw, hld_raw):
    """
    Generates a unique hash based on file contents.
    Ensures cache invalidation if code or doc changes.
    """
    combined = source_raw + "||SEPARATOR||" + hld_raw
    return hashlib.sha256(combined.encode('utf-8')).hexdigest()

def get_cached_response(cache_key, cache_dir="llm_cache"):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return None

def save_to_cache(cache_key, response_data, cache_dir="llm_cache"):
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    with open(file_path, 'w') as f:
        json.dump(response_data, f)

# ==========================================
# 6. CORE APPLICATION LOGIC
# ==========================================

def analyze_gap(client, deployment_name, source_dir, hld_path, token_report):
    """
    Main analysis function.
    """
    print(f"Reading source code from: {source_dir}")
    print(f"Reading HLD from: {hld_path}")
    
    # 1. Read Files
    code_files_dict, code_raw_content = read_source_code_files(source_dir)
    hld_content = read_hld_file(hld_path)
    
    if not code_files_dict:
        return "No source code files found.", 0

    # 2. Prepare Data Formats
    
    # A. JSON Format (For Baseline Simulation)
    json_input_str = json.dumps(code_files_dict, indent=2)
    
    # B. TOON Format (For Optimization)
    toon_input_str = dict_to_toon(code_files_dict)
    
    # 3. Calculate Baseline (Simulated)
    # We simulate what the token count WOULD have been if we sent JSON
    system_prompt = "You are an expert code auditor. Analyze the Source Code against the HLD documentation. Identify gaps."
    
    # Counting system prompt once
    sys_tokens = count_tokens_tiktoken(system_prompt)
    hld_tokens = count_tokens_tiktoken(hld_content)
    
    # Baseline Input = System Prompt + HLD + JSON Source
    json_source_tokens = count_tokens_tiktoken(json_input_str)
    baseline_input_total = sys_tokens + hld_tokens + json_source_tokens
    
    token_report['baseline_input_tokens'] = baseline_input_total
    token_report['file_count'] = len(code_files_dict)

    # 4. Caching Logic
    cache_key = generate_cache_key(code_raw_content, hld_content)
    cached_result = get_cached_response(cache_key)
    
    if cached_result:
        print("CACHE HIT: Retrieved response from LLM Cache.")
        token_report['cache_hit'] = True
        token_report['actual_input_tokens'] = 0
        token_report['output_tokens'] = 0
        token_report['total_tokens_used'] = 0
        # For cache hits, we can't show "Actual vs Baseline" for this specific run 
        # because we didn't send the prompt, but the design saving (TOON) remains constant.
        return cached_result.get("response")
    
    print("CACHE MISS: Calling Azure OpenAI...")
    token_report['cache_hit'] = False
    
    # 5. Construct Optimized Prompt
    user_prompt = f"""
    # HLD Documentation:
    {hld_content}
    
    # Source Code Data (TOON Format):
    {toon_input_str}
    
    Task: Analyze the gaps between the HLD and Source Code.
    """
    
    # 6. Call Azure OpenAI
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0 
        )
        
        response_text = response.choices[0].message.content
        
        # 7. Extract Usage from API Response (THE REQUIREMENT)
        usage = response.usage
        
        token_report['actual_input_tokens'] = usage.prompt_tokens
        token_report['output_tokens'] = usage.completion_tokens
        token_report['total_tokens_used'] = usage.total_tokens
        
        # Save to Cache
        save_to_cache(cache_key, {"response": response_text, "usage": usage.model_dump()})
        
        return response_text
        
    except Exception as e:
        print(f"Error: {str(e)}")
        return f"Error during analysis: {str(e)}"

# ==========================================
# 7. REPORT GENERATION
# ==========================================

def generate_markdown_report(report_data, analysis_result, output_file="token_usage_report.md"):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Calculate Reduction
    # Note: If Cache Hit, actual tokens are 0, reduction would be 100% regarding runtime, 
    # but we focus on 'Design Reduction' (JSON vs TOON) for the first table.
    
    tokens_saved_design = report_data['baseline_input_tokens'] - report_data.get('actual_input_tokens', 0)
    
    # Avoid division by zero if actual_input_tokens is 0 (cache hit)
    if report_data['baseline_input_tokens'] > 0:
        reduction_pct = (tokens_saved_design / report_data['baseline_input_tokens']) * 100
    else:
        reduction_pct = 0

    content = f"""# Token Usage & Optimization Report

**Date:** {now}

## 1. Executive Summary
This report presents the token usage metrics for the Code-to-HLD Gap Analysis application. 
It highlights the efficiency gains achieved through **TOON (Table Oriented Object Notation)** and **LLM Caching**.

## 2. Optimization Metrics (Input Reduction)
The following table compares the theoretical usage of standard JSON input versus the implemented TOON input.

| Metric | Token Count |
| :--- | :--- |
| **Standard JSON Input (Baseline)** | {report_data['baseline_input_tokens']} |
| **Optimized TOON Input (Actual)** | {report_data.get('actual_input_tokens', 'N/A')} |
| **Tokens Saved (Design)** | {tokens_saved_design} |
| **Input Reduction Rate** | **{reduction_pct:.2f}%** |

## 3. Execution Statistics
Details of the last API call (if cache missed) or cached retrieval.

| Metric | Value |
| :--- | :--- |
| **Source Files Analyzed** | {report_data.get('file_count', 'N/A
