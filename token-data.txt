import os
import json
import hashlib
import tiktoken
from openai import AzureOpenAI
from dotenv import load_dotenv
import datetime

# ==========================================
# 1. SETUP & CONFIGURATION
# ==========================================

def get_azure_client():
    """Initializes and returns the Azure OpenAI client."""
    load_dotenv()
    
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    return client

# ==========================================
# 2. FILE SYSTEM OPERATIONS
# ==========================================

def read_source_code_files(source_dir, extensions=[".py", ".js", ".java", ".cs", ".ts"]):
    """
    Recursively reads all files.
    Returns:
        1. code_files_dict: {filepath: content}
        2. raw_concat: Raw string for hashing
    """
    code_files = {}
    raw_content_for_hash = ""
    
    if not os.path.exists(source_dir):
        raise FileNotFoundError(f"Source directory not found: {source_dir}")

    for root, dirs, files in os.walk(source_dir):
        dirs[:] = [d for d in dirs if d not in ['venv', '__pycache__', 'node_modules', '.git', 'env', 'build', 'dist']]
        
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        rel_path = os.path.relpath(file_path, source_dir)
                        code_files[rel_path] = content
                        raw_content_for_hash += f"FILE:{rel_path}\n{content}\n"
                except Exception as e:
                    print(f"Warning: Could not read {file_path}: {e}")
                    
    return code_files, raw_content_for_hash

def read_hld_file(hld_path):
    if not os.path.exists(hld_path):
        raise FileNotFoundError(f"HLD file not found: {hld_path}")
    with open(hld_path, 'r', encoding='utf-8') as f:
        return f.read()

# ==========================================
# 3. TOON IMPLEMENTATION (OPTIMIZED)
# ==========================================

def dict_to_optimized_toon(data):
    """
    Converts dictionary to Separator-based TOON.
    Strategy: 
    - JSON uses: {"file": "line1\nline2"} -> High overhead (quotes, escapes).
    - TOON uses: ### FILE: name \n raw content -> Low overhead (natural newlines).
    """
    if not data:
        return "No data provided"
    
    rows = []
    # We use a specific separator that LLMs recognize as a header
    # This avoids the token cost of JSON braces and escaped newlines.
    separator = "#FILE_START#"
    
    for filename, content in data.items():
        # Format: SEPARATOR path \n content
        rows.append(f"{separator} {filename}")
        rows.append(content)
            
    return "\n".join(rows)

# ==========================================
# 4. TOKEN COUNTING UTILITIES
# ==========================================

def count_tokens_tiktoken(text, model="gpt-4"):
    """Used for Baseline Simulation."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# ==========================================
# 5. LLM CACHING STRATEGY
# ==========================================

def generate_cache_key(source_raw, hld_raw):
    combined = source_raw + "||SEPARATOR||" + hld_raw
    return hashlib.sha256(combined.encode('utf-8')).hexdigest()

def get_cached_response(cache_key, cache_dir="llm_cache"):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return None

def save_to_cache(cache_key, response_data, cache_dir="llm_cache"):
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    with open(file_path, 'w') as f:
        json.dump(response_data, f)

# ==========================================
# 6. CORE APPLICATION LOGIC
# ==========================================

def analyze_gap(client, deployment_name, source_dir, hld_path, token_report):
    print(f"Reading source code from: {source_dir}")
    print(f"Reading HLD from: {hld_path}")
    
    # 1. Read Files
    code_files_dict, code_raw_content = read_source_code_files(source_dir)
    hld_content = read_hld_file(hld_path)
    
    if not code_files_dict:
        return "No source code files found.", 0

    # 2. Prepare Data Formats
    
    # A. JSON Format (For Baseline Simulation)
    # We simulate the "Old Way" to prove the reduction
    json_input_str = json.dumps(code_files_dict, indent=2)
    
    # B. TOON Format (Optimized)
    # Using the new Separator-based approach
    toon_input_str = dict_to_optimized_toon(code_files_dict)
    
    # 3. Calculate Baseline (Simulation)
    system_prompt_baseline = "You are an expert code auditor. Analyze the JSON source code against the HLD."
    sys_tokens_base = count_tokens_tiktoken(system_prompt_baseline)
    hld_tokens = count_tokens_tiktoken(hld_content)
    json_source_tokens = count_tokens_tiktoken(json_input_str)
    
    baseline_input_total = sys_tokens_base + hld_tokens + json_source_tokens
    token_report['baseline_input_tokens'] = baseline_input_total
    token_report['file_count'] = len(code_files_dict)

    # 4. Caching Logic
    cache_key = generate_cache_key(code_raw_content, hld_content)
    cached_result = get_cached_response(cache_key)
    
    if cached_result:
        print("CACHE HIT: Retrieved response from LLM Cache.")
        token_report['cache_hit'] = True
        token_report['actual_input_tokens'] = 0
        token_report['output_tokens'] = 0
        token_report['total_tokens_used'] = 0
        return cached_result.get("response")
    
    print("CACHE MISS: Calling Azure OpenAI...")
    token_report['cache_hit'] = False
    
    # 5. Construct Optimized Prompt (TUNED FOR TOON)
    # We explicitly tell the LLM how to read the data
    system_prompt_opt = """You are an expert code auditor. 
The source code is provided in a compact 'TOON' format (Table Oriented Object Notation).
Each file starts with a header '#FILE_START# filename' followed immediately by the file content.
Analyze this content against the HLD documentation."""
    
    user_prompt = f"""
# HLD Documentation:
{hld_content}

# Source Code (TOON Format):
{toon_input_str}

Task: Compare the source code implementation against the HLD. Identify gaps, missing features, or logic discrepancies.
"""
    
    # 6. Call Azure OpenAI
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt_opt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0 
        )
        
        response_text = response.choices[0].message.content
        usage = response.usage
        
        # Update Report with ACTUAL usage from API
        token_report['actual_input_tokens'] = usage.prompt_tokens
        token_report['output_tokens'] = usage.completion_tokens
        token_report['total_tokens_used'] = usage.total_tokens
        
        save_to_cache(cache_key, {"response": response_text, "usage": usage.model_dump()})
        
        return response_text
        
    except Exception as e:
        print(f"Error: {str(e)}")
        return f"Error during analysis: {str(e)}"

# ==========================================
# 7. REPORT GENERATION
# ==========================================

def generate_markdown_report(report_data, analysis_result, output_file="token_usage_report.md"):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Calculate Reduction
    # Baseline is simulated JSON, Actual is TOON
    tokens_saved = report_data['baseline_input_tokens'] - report_data.get('actual_input_tokens', 0)
    
    if report_data['baseline_input_tokens'] > 0:
        reduction_pct = (tokens_saved / report_data['baseline_input_tokens']) * 100
    else:
        reduction_pct = 0

    # Correct logic for cache hit display
    actual_input_display = report_data.get('actual_input_tokens', 0)
    if report_data['cache_hit']:
        savings_status = "100% (Cache Hit)"
    else:
        savings_status = f"{reduction_pct:.2f}%"

    content = f"""# Token Usage & Optimization Report

**Date:** {now}

## 1. Executive Summary
This report demonstrates the token reduction achieved by switching from standard JSON payloads to **Separator-Based TOON** and utilizing **LLM Caching**.

## 2. Token Reduction Analysis
Comparison between theoretical JSON usage (Baseline) and Actual TOON usage.

| Metric | Token Count |
| :--- | :--- |
| **Baseline (JSON Input)** | {report_data['baseline_input_tokens']} |
| **Optimized (TOON Input)** | {actual_input_display} |
| **Tokens Saved** | {tokens_saved} |
| **Reduction Rate** | **{savings_status}** |

## 3. Why did TOON reduce tokens?
1. **Removed JSON Syntax:** We eliminated `{`, `}`, `"` characters which are plentiful in JSON code dumps.
2. **Natural Newlines:** JSON requires `\n` (2 chars) for every line break. TOON uses native newlines (1 char), saving ~50% on whitespace tokens in code.
3. **Prompt Tuning:** The prompt was updated to instruct the LLM on reading the compact format, ensuring accuracy is maintained despite lower token count.

## 4. Execution Details
| Metric | Value |
| :--- | :--- |
| **Source Files Analyzed** | {report_data.get('file_count', 'N/A')} |
| **Cache Status** | {'HIT (0 Tokens Used)' if report_data['cache_hit'] else 'MISS (API Called)'} |
| **Output Tokens** | {report_data.get('output_tokens', 0)} |
| **Total Transaction Tokens** | {report_data.get('total_tokens_used', 0)} |
"""

 with open(output_file, "w") as f:
        f.write(content)
    print(f"\nReport generated: {output_file}")
