import os
import json
import hashlib
import tiktoken
from openai import AzureOpenAI
from dotenv import load_dotenv
import datetime
import glob

# ==========================================
# 1. SETUP & CONFIGURATION
# ==========================================

def get_azure_client():
    """Initializes and returns the Azure OpenAI client."""
    load_dotenv()
    
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    return client

# ==========================================
# 2. FILE SYSTEM OPERATIONS
# ==========================================

def read_source_code_files(source_dir, extensions=[".py", ".js", ".java", ".cs"]):
    """
    Recursively reads all files with given extensions from the directory.
    Returns a dictionary {filepath: content} and the raw concatenated string for hashing.
    """
    code_files = {}
    raw_content_for_hash = ""
    
    if not os.path.exists(source_dir):
        raise FileNotFoundError(f"Source directory not found: {source_dir}")

    for root, dirs, files in os.walk(source_dir):
        # Skip common non-code directories like venv, node_modules, .git
        dirs[:] = [d for d in dirs if d not in ['venv', '__pycache__', 'node_modules', '.git', 'env']]
        
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # Store relative path to save tokens and for clarity
                        rel_path = os.path.relpath(file_path, source_dir)
                        code_files[rel_path] = content
                        raw_content_for_hash += content
                except Exception as e:
                    print(f"Warning: Could not read {file_path}: {e}")
                    
    return code_files, raw_content_for_hash

def read_hld_file(hld_path):
    """Reads the HLD document content."""
    if not os.path.exists(hld_path):
        raise FileNotFoundError(f"HLD file not found: {hld_path}")
    
    with open(hld_path, 'r', encoding='utf-8') as f:
        return f.read()

# ==========================================
# 3. TOON IMPLEMENTATION
# ==========================================

def dict_to_toon(data, headers=None):
    """
    Converts a dictionary to TOON (Table Oriented Object Notation).
    For Source Code: 'File Path | Content'
    """
    if not data:
        return "No data provided"
    
    rows = []
    
    if isinstance(data, dict):
        # Determine headers based on context
        # If keys are file paths, we label them explicitly
        sample_key = next(iter(data.keys()))
        
        # Heuristic: if content looks like code, use "File | Code"
        rows.append("File Path | Content")
        
        for key, value in data.items():
            # Escape newlines in content to keep table structure valid (optional, but cleaner for logs)
            # LLMs handle newlines inside pipe cells fine usually.
            clean_val = str(value).replace("\n", "\\n") 
            rows.append(f"{key} | {clean_val}")
            
    return "\n".join(rows)

# ==========================================
# 4. TOKEN COUNTING UTILITIES
# ==========================================

def count_tokens(text, model="gpt-4"):
    """Counts the number of tokens in a text string."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# ==========================================
# 5. LLM CACHING STRATEGY
# ==========================================

def generate_cache_key(source_content, hld_content):
    """
    Generates a unique hash based on file contents.
    Strategy: If content changes -> Hash changes -> Cache Miss.
    """
    combined = source_content + "||SEPARATOR||" + hld_content
    return hashlib.sha256(combined.encode('utf-8')).hexdigest()

def get_cached_response(cache_key, cache_dir="llm_cache"):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return None

def save_to_cache(cache_key, response_data, cache_dir="llm_cache"):
    file_path = os.path.join(cache_dir, f"{cache_key}.json")
    with open(file_path, 'w') as f:
        json.dump(response_data, f)

# ==========================================
# 6. CORE APPLICATION LOGIC
# ==========================================

def analyze_gap(client, deployment_name, source_dir, hld_path, token_report):
    """
    Main analysis function.
    """
    print(f"Reading source code from: {source_dir}")
    print(f"Reading HLD from: {hld_path}")
    
    # 1. Read Files
    code_files_dict, code_raw_content = read_source_code_files(source_dir)
    hld_content = read_hld_file(hld_path)
    
    if not code_files_dict:
        return "No source code files found."

    # 2. Prepare TOON Data
    # Convert source code dictionary to TOON string
    source_toon = dict_to_toon(code_files_dict)
    
    # 3. Baseline Calculation (What it would have been in JSON)
    # This simulates dumping the whole dictionary as JSON
    json_baseline_input = json.dumps(code_files_dict)
    baseline_tokens = count_tokens(json_baseline_input)
    
    # 4. Optimized Input (TOON)
    optimized_tokens = count_tokens(source_toon)
    
    # Update report with input stats
    token_report['baseline_input_tokens'] = baseline_tokens
    token_report['optimized_input_tokens'] = optimized_tokens
    token_report['input_tokens_saved'] = baseline_tokens - optimized_tokens
    
    # 5. Caching Logic
    # Key is generated from actual file content, not the TOON string, 
    # to ensure semantic identity.
    cache_key = generate_cache_key(code_raw_content, hld_content)
    cached_result = get_cached_response(cache_key)
    
    if cached_result:
        print("CACHE HIT: Retrieved response from LLM Cache (Cost: $0.00).")
        token_report['output_tokens'] = 0
        token_report['total_tokens_used'] = 0
        token_report['cache_hit'] = True
        return cached_result.get("response")
    
    print("CACHE MISS: Calling Azure OpenAI...")
    token_report['cache_hit'] = False
    
    # 6. Construct Prompt
    system_prompt = "You are an expert code auditor. Analyze the Source Code (in TOON format) against the HLD documentation. Identify implementation gaps, missing features, or logic discrepancies."
    
    user_prompt = f"""
    # HLD Documentation:
    {hld_content}
    
    # Source Code Data (TOON Format: File Path | Content):
    {source_toon}
    
    Task: Please provide a detailed gap analysis report.
    """
    
    # Add system prompt tokens to the counts for accurate reporting
    system_tokens = count_tokens(system_prompt)
    token_report['baseline_input_tokens'] += system_tokens
    token_report['optimized_input_tokens'] += system_tokens
    
    # 7. Call Azure OpenAI
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0 
        )
        
        response_text = response.choices[0].message.content
        usage = response.usage
        
        token_report['output_tokens'] = usage.completion_tokens
        token_report['total_tokens_used'] = usage.total_tokens
        
        # Save to Cache
        save_to_cache(cache_key, {"response": response_text, "usage": usage.model_dump()})
        
        return response_text
        
    except Exception as e:
        return f"Error: {str(e)}"

# ==========================================
# 7. REPORT GENERATION
# ==========================================

def generate_markdown_report(report_data, analysis_result, output_file="token_usage_report.md"):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Calculate costs (Standard GPT-4 Pricing approximation)
    input_cost = (report_data['optimized_input_tokens'] / 1000) * 0.03
    output_cost = (report_data['output_tokens'] / 1000) * 0.06
    total_cost = input_cost + output_cost
    
    baseline_input_cost = (report_data['baseline_input_tokens'] / 1000) * 0.03
    
    content = f"""# Token Usage & Optimization Report

**Generated At:** {now}

## 1. Execution Summary
- **Source Path:** `{report_data.get('source_path', 'N/A')}`
- **HLD Path:** `{report_data.get('hld_path', 'N/A')}`
- **Files Analyzed:** {report_data.get('file_count', 'N/A')}
- **Cache Status:** {'HIT (Free)' if report_data['cache_hit'] else 'MISS (API Called)'}

## 2. Token Optimization Analysis (JSON vs TOON)
We replaced standard JSON serialization with TOON (Table Oriented Object Notation) for transmitting source code content.

| Metric | Token Count | Estimated Cost |
| :--- | :--- | :--- |
| **Baseline (JSON)** | {report_data['baseline_input_tokens']} | ${baseline_input_cost:.4f} |
| **Optimized (TOON)** | {report_data['optimized_input_tokens']} | ${input_cost:.4f} |
| **Savings** | **{report_data['input_tokens_saved']}** | **{((report_data['input_tokens_saved']/1000)*0.03):.4f}** |

## 3. Total Transaction Cost
| Item | Tokens |
| :--- | :--- |
| **Input Tokens (Sent)** | {report_data['optimized_input_tokens']} |
| **Output Tokens (Received)** | {report_data['output_tokens']} |
| **Total Tokens** | {report_data['total_tokens_used']} |
| **Total API Cost** | **${total_cost:.4f}** |

## 4. LLM Gap Analysis Result
{analysis_result}

## 5. Optimization Strategy Explanation
1. **TOON:** By removing JSON syntax (`{{`, `}}`, quotes) and using pipe-delimited tables, we reduced the input token count significantly.
2. **Caching:** A SHA-256 hash was generated from file contents. If files do not change, the cache returns the previous result, reducing cost to zero for subsequent runs.
"""
    
    with open(output_file, "w") as f:
        f.write(content)
    print(f"\nReport generated: {output_file}")

# ==========================================
# 8. MAIN EXECUTION
# ==========================================

def main():
    # --- CONFIGURATION ---
    # Replace these with your actual local paths
    SOURCE_CODE_DIR = "./sample_source_code" 
    HLD_FILE_PATH = "./sample_hld/hld.md"
    DEPLOYMENT_NAME = os.getenv("AZURE_DEPLOYMENT_NAME", "gpt-4")
    
    # Create dummy files for demonstration if they don't exist
    if not os.path.exists(SOURCE_CODE_DIR):
        os.makedirs(SOURCE_CODE_DIR)
        with open(os.path.join(SOURCE_CODE_DIR, "main.py"), "w") as f:
            f.write("def add(a, b):\n    return a + b\n")
        with open(os.path.join(SOURCE_CODE_DIR, "utils.py"), "w") as f:
            f.write("def delete_user(id):\n    pass # TODO\n")
            
    if not os.path.exists(os.path.dirname(HLD_FILE_PATH)):
        os.makedirs(os.path.dirname(HLD_FILE_PATH))
    if not os.path.exists(HLD_FILE_PATH):
        with open(HLD_FILE_PATH, "w") as f:
            f.write("# HLD\nSystem must implement add function and user deletion logic.")

    # Initialize Client
    try:
        client = get_azure_client()
    except Exception as e:
        print(f"Client Init Error: {e}")
        return

    # Initialize Report Dictionary
    token_report = {
        'baseline_input_tokens': 0,
        'optimized_input_tokens': 0,
        'input_tokens_saved': 0,
        'output_tokens': 0,
        'total_tokens_used': 0,
        'cache_hit': False,
        'source_path': SOURCE_CODE_DIR,
        'hld_path': HLD_FILE_PATH,
        'file_count': 0
    }
    
    # Run Analysis
    print("--- Starting Gap Analysis ---")
    
    # Get file count for report
    files_dict, _ = read_source_code_files(SOURCE_CODE_DIR)
    token_report['file_count'] = len(files_dict)
    
    result = analyze_gap(
        client, 
        DEPLOYMENT_NAME, 
        SOURCE_CODE_DIR, 
        HLD_FILE_PATH, 
        token_report
    )
    
    print("\n--- Analysis Complete ---")
    
    # Generate Report
    generate_markdown_report(token_report, result)

if __name__ == "__main__":
    main()
